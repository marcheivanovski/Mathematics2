{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NelderMead import minimize\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def GD(gradient_function, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[np.array(x1)]\n",
    "    for i in range(1,steps+1):\n",
    "        x=all_points[i-1]-gamma*gradient_function(all_points[i-1])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def PGD(gradient_function, gamma, mu, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1, x1]\n",
    "    x=x1\n",
    "    for i in range(2,steps+2):\n",
    "        x = all_points[i-1]-gamma*gradient_function(all_points[i-1])+mu*(all_points[i-1]-all_points[i-2])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def NGD(gradient_function, gamma, mu, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1, x1]\n",
    "    x=x1\n",
    "    for i in range(2,steps+2):\n",
    "        x = all_points[i-1] - \\\n",
    "            gamma*gradient_function(all_points[i-1] + mu*(all_points[i-1]-all_points[i-2])) + \\\n",
    "            mu*(all_points[i-1]-all_points[i-2])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def AGD(gradient_function, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1]\n",
    "    all_gradients=np.ones(len(x1))\n",
    "    \n",
    "    for i in range(1,steps+1):\n",
    "        gradient_step=gradient_function(all_points[i-1])\n",
    "        D=np.diag(1/np.sqrt(all_gradients))\n",
    "        x = all_points[i-1] - gamma*D*gradient_step\n",
    "            \n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_gradients+=gradient_step**2\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "\n",
    "def NewtonMethod(hessian_matrix, gradient_function, x1, steps, timelimit=None):\n",
    "    #all_points=np.zeros((steps+1, len(x1)))\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1]\n",
    "    for i in range(1,steps+1):\n",
    "        x=all_points[i-1]-np.dot(np.linalg.inv(hessian_matrix(all_points[i-1])),gradient_function(all_points[i-1]))\n",
    "        #print(x)\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        #all_points[i,:]=x\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "\n",
    "def BFGS(gradient_function, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    B=np.eye(len(x1))\n",
    "    all_points=[x1, x1-np.dot(B,gradient_function(x1))]\n",
    "    \n",
    "    for i in range(steps):\n",
    "        xk, xk_1 = all_points[-1], all_points[-2]\n",
    "        grad_xk, grad_xk_1 = gradient_function(xk), gradient_function(xk_1)\n",
    "        \n",
    "        gamma = grad_xk-grad_xk_1\n",
    "        delta = xk-xk_1\n",
    "        \n",
    "        #print(all_points)\n",
    "        #print(grad_xk)\n",
    "        #print(grad_xk_1)\n",
    "        gamma=gamma[..., None]\n",
    "        delta=delta[..., None]\n",
    "        \n",
    "        if delta.T.dot(gamma) == 0:\n",
    "            print('    BFGS OVERFLOW!!!')\n",
    "            return all_points\n",
    "        \n",
    "        B_new = B - (delta.dot(gamma.T.dot(B)) + B.dot(gamma).dot(delta.T)) / (delta.T.dot(gamma)) + \\\n",
    "             (1 + (gamma.T.dot(B).dot(gamma)) / (delta.T.dot(gamma))) * (delta * delta.T) / (delta.T.dot(gamma))      \n",
    "        \n",
    "        all_points.append(xk-B_new.dot(gradient_function(xk)))\n",
    "        B=B_new\n",
    "        \n",
    "        #print(time.time() - t)\n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compare them all using different number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commpare_all(fun, hessian_function, gradient_function, x1, gamma, mu, actual_min, delta):\n",
    "    \n",
    "    print(f\"Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    print(f\"    NelderMead method: {np.sum(np.square(minimize(fun, x1, max_iterations=10, delta=delta)- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    NelderMead method: {np.sum(np.square(minimize(fun, x1, max_iterations=100, delta=delta)- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    NelderMead method: {np.sum(np.square(minimize(fun, x1, max_iterations=1000, delta=delta)- actual_min))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First function\n",
    "The first function on which we are going to compare all approaches is:\n",
    "$$f(x,y,z)=(x-z)^{2}+(2y+z)^{2}+(4x-2y+z)^{2}+x+y$$\n",
    "First we will start with \n",
    "$$x_{1}=(0,0,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.05475\n",
      "    Polyak gradient: 0.05305\n",
      "    Nestorov gradient descend: 0.05311\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS method: 8.520524277920641e-16\n",
      "    NelderMead method: 0.0015423657682602562\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00027\n",
      "    Polyak gradient: 0.00019\n",
      "    Nestorov gradient descend: 0.00020\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "    NelderMead method: 9.887765520216837e-10\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00000\n",
      "    Polyak gradient: 0.00000\n",
      "    Nestorov gradient descend: 0.00000\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "    NelderMead method: 9.887765520216837e-10\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_a_part(X):\n",
    "    x,y,z = X[0], X[1], X[2]\n",
    "    return np.array([34*x-16*y+6*z+1, -16*x+16*y+1, 6*x+6*z])\n",
    "\n",
    "def hessian_function_a_part(_):\n",
    "    return np.array([\n",
    "        np.array([34, -16, 6]),\n",
    "        np.array([-16, 16, 0]),\n",
    "        np.array([6, 0, 6]),\n",
    "    ])\n",
    "\n",
    "def fun_a_part(X):\n",
    "    x,y,z = X[0], X[1], X[2]\n",
    "    return (x-z)**2+(2*y+z)**2+(4*x-2*y+z)**2+x+y\n",
    "\n",
    "gamma = 0.01\n",
    "mu = 0.05\n",
    "x1=np.array([0,0,0])\n",
    "actual_min = np.array([-1/6, -11/48, 1/6])\n",
    "delta=0.1\n",
    "\n",
    "commpare_all(fun_a_part,hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then \n",
    "$$x_{1}=(1,1,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.99750\n",
      "    Polyak gradient: 0.95503\n",
      "    Nestorov gradient descend: 0.95750\n",
      "    Newton method: 2.311115933264683e-33\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "    NelderMead method: 1.6594534226870836\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00407\n",
      "    Polyak gradient: 0.00295\n",
      "    Nestorov gradient descend: 0.00298\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "    NelderMead method: 1.7830876131816685e-09\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00000\n",
      "    Polyak gradient: 0.00000\n",
      "    Nestorov gradient descend: 0.00000\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "    NelderMead method: 1.7830876131816685e-09\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "mu = 0.05\n",
    "x1=np.array([1,1,0])\n",
    "actual_min = np.array([-1/6, -11/48, 1/6])\n",
    "delta=0.1\n",
    "\n",
    "commpare_all(fun_a_part,hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second function\n",
    "\n",
    "The second function on which we are going to compare all approaches is:\n",
    "$$f(x,y,z)=(x-1)^{2}+(y-1)^{2}+100(y-x^{2})^{2}+100(z-y^{2})^{2}$$\n",
    "First we will start with \n",
    "$$x_{1}=(1.2, 1.2, 1.2)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.10314\n",
      "    Polyak gradient: 0.10160\n",
      "    Nestorov gradient descend: 0.10161\n",
      "    Newton method: 3.5664062500000004\n",
      "    BFGS method: 3.5664062500000004\n",
      "    NelderMead method: 0.1489573066436348\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.03639\n",
      "    Polyak gradient: 0.03589\n",
      "    Nestorov gradient descend: 0.03587\n",
      "    Newton method: 3.56641\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 3.56641\n",
      "    NelderMead method: 2.106808024003783e-07\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 1.10751\n",
      "    Polyak gradient: 1.26815\n",
      "    Nestorov gradient descend: 1.26799\n",
      "    Newton method: 3.56641\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 3.56641\n",
      "    NelderMead method: 4.028426536812032e-09\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_b_part(X):\n",
    "    x, y, z = X[0], X[1], X[2]\n",
    "    return np.array([2*(x-1)-400*x*(y-x**2), 2*(y-1)+200*(y-x**2)-400*y*(z-y**2), 200*(z-y**2)])\n",
    "\n",
    "def hessian_function_b_part(X):\n",
    "    x, y, z = X[0], X[1], X[2]\n",
    "    return np.array([\n",
    "        [-400*(y-x**2) + 800*x**2 + 2, -400*x, 0],\n",
    "        [-400*x, -400*(z-y**2) + 800*y**2 + 202, -400*y],\n",
    "        [0, -400*y, 200]\n",
    "    ])\n",
    "\n",
    "def fun_b_part(X):\n",
    "    x, y, z = X[0], X[1], X[2]\n",
    "    return (x-1)**2+(y-1)**2+100*(y-x**2)**2+100*(z-y**2)**2\n",
    "\n",
    "gamma = 0.0001\n",
    "mu = 0.1\n",
    "x1=np.array([1.2, 1.2, 1.2])\n",
    "actual_min = np.array([1, 1, 1])\n",
    "\n",
    "delta=0.1\n",
    "\n",
    "commpare_all(fun_b_part,hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then,\n",
    "$$x_{1}=(-1, 1.2, 1.2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 3.89218\n",
      "    Polyak gradient: 3.87428\n",
      "    Nestorov gradient descend: 3.87437\n",
      "    Newton method: 3.5664062500000004\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 3.5664062500000004\n",
      "    NelderMead method: 4.339876543209876\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 2.73333\n",
      "    Polyak gradient: 2.64018\n",
      "    Nestorov gradient descend: 2.64065\n",
      "    Newton method: 3.56641\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 3.56641\n",
      "    NelderMead method: 2.0383914617433274\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 2.27616\n",
      "    Polyak gradient: 2.38798\n",
      "    Nestorov gradient descend: 2.38788\n",
      "    Newton method: 3.56641\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 3.56641\n",
      "    NelderMead method: 2.8779533936226077e-09\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([-1, 1.2, 1.2])\n",
    "gamma = 0.0001\n",
    "mu = 0.1\n",
    "actual_min = np.array([1, 1, 1])\n",
    "delta=0.1\n",
    "\n",
    "commpare_all(fun_b_part,hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third function on which we are going to compare all approaches is:\n",
    "$$(1.5-x+xy)^{2}+(2.25-x+xy^{2})^{2}+(2.625-x+xy^{3})^{2}$$\n",
    "First we will start with \n",
    "$$x_{1}=(1, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.22247\n",
      "    Polyak gradient: 4.21979\n",
      "    Nestorov gradient descend: 4.21978\n",
      "    Newton method: 9.25\n",
      "    BFGS method: nan\n",
      "    NelderMead method: 0.5049041771888727\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 3.98408\n",
      "    Polyak gradient: 3.95600\n",
      "    Nestorov gradient descend: 3.95591\n",
      "    Newton method: 9.25000\n",
      "    BFGS method: nan\n",
      "    NelderMead method: 3.4633924853168536e-08\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 2.35085\n",
      "    Polyak gradient: 2.22826\n",
      "    Nestorov gradient descend: 2.22818\n",
      "    Newton method: 9.25000\n",
      "    BFGS method: nan\n",
      "    NelderMead method: 3.4633924853168536e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d62d6bc5e640>:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.array([2*(1.5-x+x*y)*(y-1)+2*(2.25-x+x*y**2)*(y**2-1)+2*(2.625-x+x*y**3)*(y**3-1), \\\n",
      "<ipython-input-13-d62d6bc5e640>:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*(x*y)+6*(2.625-x+x*y**3)*(x*y**2)])\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_c_part(X):\n",
    "    x, y = X[0], X[1]\n",
    "    return np.array([2*(1.5-x+x*y)*(y-1)+2*(2.25-x+x*y**2)*(y**2-1)+2*(2.625-x+x*y**3)*(y**3-1), \\\n",
    "                    2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*(x*y)+6*(2.625-x+x*y**3)*(x*y**2)])\n",
    "\n",
    "def hessian_matrix_c_part(X):\n",
    "    x, y = X[0], X[1]\n",
    "    return np.array([[2*(y**6+y**4-2*y**3-y**2-2*y+3), 2*x*(6*y**5+4*y**3-6*y**2-2*y-2)+15.75*y**2+9*y+3], \\\n",
    "                    [2*x*(6*y**5+4*y**3-6*y**2-2*y-2)+15.75*y**2+9*y+3, 2*x*(x*(15*y**4+6*y**2-6*y-1)+6*2.625*y+4.5*y)]])\n",
    "\n",
    "def fun_c_part(X):\n",
    "    x,y = X[0], X[1]\n",
    "    return (1.5-x+x*y)**2+(2.25-x+x*y**2)**2+(2.625-x+x*y**3)**2\n",
    "\n",
    "\n",
    "gamma = 0.0001\n",
    "mu = 0.1\n",
    "x1=np.array([1,1])\n",
    "actual_min = np.array([3, 0.5])\n",
    "delta = 0.1\n",
    "\n",
    "commpare_all(fun_c_part, hessian_matrix_c_part, gradient_function_c_part, x1, gamma, mu, actual_min, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then\n",
    "$$x_{1}=(4.5, 4.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 9.82881\n",
      "    Polyak gradient: 6.85520\n",
      "    Nestorov gradient descend: 7.32214\n",
      "    Newton method: 8.582711040688768\n",
      "    BFGS method: nan\n",
      "    NelderMead method: 4.025334472656165\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 3.58792\n",
      "    Polyak gradient: 2.41752\n",
      "    Nestorov gradient descend: 2.46649\n",
      "    Newton method: 9.25000\n",
      "    BFGS method: nan\n",
      "    NelderMead method: 5.2163738614556345e-09\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 1.14620\n",
      "    Polyak gradient: 0.83584\n",
      "    Nestorov gradient descend: 0.85289\n",
      "    Newton method: 9.25000\n",
      "    BFGS method: nan\n",
      "    NelderMead method: 5.2163738614556345e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d62d6bc5e640>:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return np.array([2*(1.5-x+x*y)*(y-1)+2*(2.25-x+x*y**2)*(y**2-1)+2*(2.625-x+x*y**3)*(y**3-1), \\\n",
      "<ipython-input-13-d62d6bc5e640>:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*(x*y)+6*(2.625-x+x*y**3)*(x*y**2)])\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-6\n",
    "mu = 0.5\n",
    "x1=np.array([4.5, 4.5])\n",
    "actual_min = np.array([3, 0.5])\n",
    "delta = 0.1\n",
    "\n",
    "commpare_all(fun_c_part, hessian_matrix_c_part, gradient_function_c_part, x1, gamma, mu, actual_min, delta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9735b1c647ce1dfbc3e02e80d081f3fd5a287ba20a216762fe185bc26e0aac9"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ids-clone] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
