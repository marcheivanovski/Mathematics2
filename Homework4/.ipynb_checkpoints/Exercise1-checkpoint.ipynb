{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all functions first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NelderMead import minimize\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def GD(gradient_function, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[np.array(x1)]\n",
    "    for i in range(1,steps+1):\n",
    "        x=all_points[i-1]-gamma*gradient_function(all_points[i-1])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def PGD(gradient_function, gamma, mu, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1, x1]\n",
    "    x=x1\n",
    "    for i in range(2,steps+2):\n",
    "        x = all_points[i-1]-gamma*gradient_function(all_points[i-1])+mu*(all_points[i-1]-all_points[i-2])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def NGD(gradient_function, gamma, mu, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1, x1]\n",
    "    x=x1\n",
    "    for i in range(2,steps+2):\n",
    "        x = all_points[i-1] - \\\n",
    "            gamma*gradient_function(all_points[i-1] + mu*(all_points[i-1]-all_points[i-2])) + \\\n",
    "            mu*(all_points[i-1]-all_points[i-2])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def AGD(gradient_function, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1]\n",
    "    all_gradients=np.ones(len(x1))\n",
    "    \n",
    "    for i in range(1,steps+1):\n",
    "        gradient_step=gradient_function(all_points[i-1])\n",
    "        D=np.diag(1/np.sqrt(all_gradients))\n",
    "        x = all_points[i-1] - gamma*D*gradient_step\n",
    "            \n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_gradients+=gradient_step**2\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "\n",
    "def NewtonMethod(hessian_matrix, gradient_function, x1, steps, timelimit=None):\n",
    "    #all_points=np.zeros((steps+1, len(x1)))\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1]\n",
    "    for i in range(1,steps+1):\n",
    "        x=all_points[i-1]-np.dot(np.linalg.inv(hessian_matrix(all_points[i-1])),gradient_function(all_points[i-1]))\n",
    "        #print(x)\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        #all_points[i,:]=x\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "\n",
    "def BFGS(gradient_function, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    B=np.eye(len(x1))\n",
    "    all_points=[x1, x1-np.dot(B,gradient_function(x1))]\n",
    "    \n",
    "    for i in range(steps):\n",
    "        xk, xk_1 = all_points[-1], all_points[-2]\n",
    "        grad_xk, grad_xk_1 = gradient_function(xk), gradient_function(xk_1)\n",
    "        \n",
    "        gamma = grad_xk-grad_xk_1\n",
    "        delta = xk-xk_1\n",
    "        \n",
    "        #print(all_points)\n",
    "        #print(grad_xk)\n",
    "        #print(grad_xk_1)\n",
    "        gamma=gamma[..., None]\n",
    "        delta=delta[..., None]\n",
    "        \n",
    "        if delta.T.dot(gamma) == 0:\n",
    "            print('    BFGS OVERFLOW!!!')\n",
    "            return all_points\n",
    "        \n",
    "        B_new = B - (delta.dot(gamma.T.dot(B)) + B.dot(gamma).dot(delta.T)) / (delta.T.dot(gamma)) + \\\n",
    "             (1 + (gamma.T.dot(B).dot(gamma)) / (delta.T.dot(gamma))) * (delta * delta.T) / (delta.T.dot(gamma))      \n",
    "        \n",
    "        all_points.append(xk-B_new.dot(gradient_function(xk)))\n",
    "        B=B_new\n",
    "        \n",
    "        #print(time.time() - t)\n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to compare them all using different number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commpare_all(fun, hessian_function, gradient_function, x1, gamma, mu, actual_min, delta):\n",
    "    \n",
    "    print(f\"Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    print(f\"    NelderMead method: {np.sum(np.square(minimize(fun, x1, max_iterations=10, delta=delta)- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    NelderMead method: {np.sum(np.square(minimize(fun, x1, max_iterations=100, delta=delta)- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 1000)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    NelderMead method: {np.sum(np.square(minimize(fun, x1, max_iterations=1000, delta=delta)- actual_min))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First function\n",
    "The first function on which we are going to compare all approaches is:\n",
    "$$f(x,y,z)=(x-z)^{2}+(2y+z)^{2}+(4x-2y+z)^{2}+x+y$$\n",
    "First we will start with \n",
    "$$x_{1}=(0,0,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.05475\n",
      "    Polyak gradient: 0.05305\n",
      "    Nestorov gradient descend: 0.05311\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS method: 8.520524277920641e-16\n",
      "2\n",
      "    NelderMead method: 59.63368055555556\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00027\n",
      "    Polyak gradient: 0.00019\n",
      "    Nestorov gradient descend: 0.00020\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "2\n",
      "    NelderMead method: 59.63368055555556\n",
      "Testing all the methods on 1000 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00000\n",
      "    Polyak gradient: 0.00000\n",
      "    Nestorov gradient descend: 0.00000\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "2\n",
      "    NelderMead method: 59.63368055555556\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_a_part(X):\n",
    "    x,y,z = X[0], X[1], X[2]\n",
    "    return np.array([34*x-16*y+6*z+1, -16*x+16*y+1, 6*x+6*z])\n",
    "\n",
    "def hessian_function_a_part(_):\n",
    "    return np.array([\n",
    "        np.array([34, -16, 6]),\n",
    "        np.array([-16, 16, 0]),\n",
    "        np.array([6, 0, 6]),\n",
    "    ])\n",
    "\n",
    "def fun(X):\n",
    "    x,y,z = X[0], X[1], X[2]\n",
    "    return (x-z)**2+(2*y+z)**2+(4*x-2*y+z)**2+x+y\n",
    "\n",
    "gamma = 0.01\n",
    "mu = 0.05\n",
    "x1=np.array([0,0,0])\n",
    "actual_min = np.array([-1/6, -11/48, 1/6])\n",
    "delta=5\n",
    "\n",
    "commpare_all(fun,hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min, delta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ids-clone] *",
   "language": "python",
   "name": "conda-env-ids-clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
