print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
minus_logf <- function(x) {
return (-log(p_scenario1(c(x))))
}
minus_logf_grad <- function(x) {
mu <- c(0,0)
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
return ((sigma%*%(x-mu)))
}
minus_logf <- function(x) {
return (-log(p_scenario1(c(x))))
}
minus_logf_grad <- function(x) {
mu <- c(0,0)
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
return ((sigma%*%(x-mu)))
}
L = 20
epsilon = 0.5
#current_q = c(0,0)
current_q = c(0.5,1)
m = 100
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
ess(samples)
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
## HMC
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
minus_logf_grad <- function(x) {
mu <- c(0,0)
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
return (-1*(sigma%*%(x-mu)))
}
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
den3d <- kde2d(samples[,1], samples[,2])
ess(samples)
minus_logf <- function(x) {
return (-log(p_scenario1(c(x))))
}
minus_logf_grad <- function(x) {
mu <- c(0,0)
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
return ((sigma%*%(x-mu)))
}
L = 20
L = 10
epsilon = 0.5
#current_q = c(0,0)
current_q = c(0.5,1)
m = 100
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
## HMC
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
L = 10
epsilon = 0.5
#current_q = c(0,0)
current_q = c(0.5,1)
m = 100
m = 1000
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
ess(samples)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()
acf(samples, lag.max = 100)
print("MCMC standard error:")
#mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
acf(samples, lag.max = 200)
B <- 0.05
minus_logf <- function(x) {
-log(p_scenario4(c(x)))
}
minus_logf_grad <- function(x) {
a <- unname(as.matrix(df[, names(data_log_reg) != "y"]))
y <- df$y
paste0(length(x), nrow(t(a)))
reg <- t(x %*% t(a))
sigm <- 1/(1+exp(-reg))
return (c((t((y - sigm)) %*% a)))
}
## HMC
L = 20
epsilon = 0.001
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
m = 1000
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
## HMC
L = 20
epsilon = 0.0001
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
m = 1000
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
epsilon = 0.01
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
m = 1000
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
epsilon = 0.009
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
epsilon = 0.0009
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
m = 1000
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
## HMC
L = 50
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
## HMC
L = 70
epsilon = 0.0009
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
m = 1000
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
current_q = c(0,0,0,0,0,0,0,0,0,0,0)#c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
acf(samples, lag.max = 100)
acf(samples, lag.max = 100)
paste0("Mean of samples (",colMeans(samples),")")
paste0(colMeans(samples))
metropolis_hastings <- function(f, m, x0, cov, dimensions){
#New code
samples <- matrix(vector('integer', dimensions*m), ncol = dimensions)
samples[1,] <- x0
for(i in 2:m){
prev_x <-  samples[i-1,]
new_x <- rmvnorm(1, prev_x, cov)
alpha <- min(1, (f(c(new_x))*dmvnorm(new_x, prev_x, cov))/(f(c(prev_x))*dmvnorm(prev_x, new_x, cov)) )
u <- runif(1)
if (is.nan(alpha)){
samples[i,]<-prev_x
}
else if (u<=alpha){
samples[i,]<-new_x
}
else{
samples[i,]<-prev_x
}
}
return (samples)
}
n <- 11
m<-1000
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.02*diag(11), 11)
acf(samples, lag.max = 100)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.002*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.0002*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.002*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.005*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.009*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.005*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.0055*diag(11), 11)
ess(samples)
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.005*diag(11), 11)
ess(samples)
print(colMeans(samples))
p_scenario4_mean()
#df <- read.csv("datset.csv", header = TRUE, sep = ",")
#model  <- glm(y ~ X2, family="binomial", data=df)
#y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
y_pred <- as.matrix(df[,1:2]) %*% x
p_scenario1 <- function(x){
mu <- c(0,0)
sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
return (dmvnorm(x, mu, sigma))
}
p_scenario2 <- function(x) {
B <- 0.05
exp( (-(x[1]^2)/200- 0.5 * (x[2]+ B * x[1]^2 - 100*B)^2 ) )
}
df <- read.csv("datset.csv", header = TRUE, sep = ",")
#x are just the fit coefficients
p_scenario3 <- function(x){
#df <- read.csv("datset.csv", header = TRUE, sep = ",")
#model  <- glm(y ~ X2, family="binomial", data=df)
#y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
y_pred <- as.matrix(df[,1:2]) %*% x
y_pred <- 1/(1+exp(-y_pred))
return (prod(lr_likelihood(df$y, y_pred)))
}
p_scenario4 <- function(x){
#model  <- glm(y ~ X2+X3+X4+X5+X6+X7+X8+X9+X10+X11, family="binomial", data=df)
#y_pred <- predict(model, df[,2:11], type="response")
#print(x)
#print("---")
y_pred <- as.matrix(df[,1:11]) %*% x
y_pred <- 1/(1+exp(-y_pred))
return (prod(lr_likelihood(df$y, y_pred)))
}
p_scenario3_mean <- function(){
#df <- read.csv("datset.csv", header = TRUE, sep = ",")
model  <- glm(y ~ X2, family="binomial", data=df)
#print(summary(model))
#y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
return (model$coefficients)
}
p_scenario4_mean <- function(){
#df <- read.csv("datset.csv", header = TRUE, sep = ",")
model  <- glm(y ~ X2+X3+X4+X5+X6+X7+X8+X9+X10+X11, family="binomial", data=df)
#print(summary(model))
#y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
return (model$coefficients)
}
envelope_multi_norm <- function(x){
mu <- rep(0, length(x))
sigma <- diag(nrow=length(x))
return (dmvnorm(x, mu, sigma))
}
envelope_multi_norm_sampler <- function(n){
g_mu <- rep(0, n)
g_sigma <- diag(nrow=n)
return (c(rmvnorm(1, g_mu, g_sigma)))
}
envelope_uniform <- function(x){
return (1)
}
envelope_uniform_sampler <- function(n){
return (c(runif(n, -20, 20)))
}
calculate_M <- function(f, g, g_sampler, n){
samples <- matrix(vector('numeric', length=10000*n), ncol=n)
for (i in 1:10000){
samples[i,]<-g_sampler(n)
}
g_values <- apply(samples, 1, g)
f_values <- apply(samples, 1, f)
M <- max(f_values/g_values, na.rm = T)
return (M)
}
rejection_sampling <- function (f, g, g_sampler, M, m, n){
samples <- matrix(vector('integer', 2*m), ncol = 2)
for(i in 1:m){
print(paste("On sample", i))
repeat{
y <- g_sampler(n) #2 is the dimension of bivariate
u <- runif(1)
#print(u)
#print((f(y)/(M*g(y))))
#print("----")
if ( u <= (f(y)/(M*g(y))) ){
break
}
else{
#print("nope")
}
}
samples[i,] <- y
}
return (samples)
}
#samples <- rejection_sampling(p_scenario1, envelope_multi_norm, envelope_multi_norm_sampler, 5, 10000)
n <- 2
m <- 100
## Uniform envelope
M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)
m <- 10000
## Uniform envelope
M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)
m <- 1000
## Uniform envelope
M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)
n <- 2
m <- 1000
## Uniform envelope
M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)
## Bivariate normal envelope
#M <- calculate_M(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, n)
#samples <- rejection_sampling(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, M, m, n)
#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
# traceplot
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
n <- 2
m <- 1000
## Uniform envelope
M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)
## Bivariate normal envelope
#M <- calculate_M(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, n)
#samples <- rejection_sampling(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, M, m, n)
#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
# traceplot
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
envelope_multi_norm <- function(x){
mu <- rep(0, length(x))
sigma <- diag(nrow=length(x))
return (dmvnorm(x, mu, sigma))
}
envelope_multi_norm_sampler <- function(n){
g_mu <- rep(0, n)
g_sigma <- diag(nrow=n)
return (c(rmvnorm(1, g_mu, g_sigma)))
}
envelope_uniform <- function(x){
return (1)
}
envelope_uniform_sampler <- function(n){
return (c(runif(n, -50, 50)))
}
calculate_M <- function(f, g, g_sampler, n){
samples <- matrix(vector('numeric', length=10000*n), ncol=n)
for (i in 1:10000){
samples[i,]<-g_sampler(n)
}
g_values <- apply(samples, 1, g)
f_values <- apply(samples, 1, f)
M <- max(f_values/g_values, na.rm = T)
return (M)
}
rejection_sampling <- function (f, g, g_sampler, M, m, n){
samples <- matrix(vector('integer', 2*m), ncol = 2)
for(i in 1:m){
print(paste("On sample", i))
repeat{
y <- g_sampler(n) #2 is the dimension of bivariate
u <- runif(1)
#print(u)
#print((f(y)/(M*g(y))))
#print("----")
if ( u <= (f(y)/(M*g(y))) ){
break
}
else{
#print("nope")
}
}
samples[i,] <- y
}
return (samples)
}
#samples <- rejection_sampling(p_scenario1, envelope_multi_norm, envelope_multi_norm_sampler, 5, 10000)
envelope_multi_norm <- function(x){
mu <- rep(0, length(x))
sigma <- diag(nrow=length(x))
return (dmvnorm(x, mu, sigma))
}
envelope_multi_norm_sampler <- function(n){
g_mu <- rep(0, n)
g_sigma <- diag(nrow=n)
return (c(rmvnorm(1, g_mu, g_sigma)))
}
envelope_uniform <- function(x){
return (1)
}
envelope_uniform_sampler <- function(n){
return (c(runif(n, -50, 50)))
}
calculate_M <- function(f, g, g_sampler, n){
samples <- matrix(vector('numeric', length=10000*n), ncol=n)
for (i in 1:10000){
samples[i,]<-g_sampler(n)
}
g_values <- apply(samples, 1, g)
f_values <- apply(samples, 1, f)
M <- max(f_values/g_values, na.rm = T)
return (M)
}
rejection_sampling <- function (f, g, g_sampler, M, m, n){
samples <- matrix(vector('integer', 2*m), ncol = 2)
for(i in 1:m){
#print(paste("On sample", i))
repeat{
y <- g_sampler(n) #2 is the dimension of bivariate
u <- runif(1)
#print(u)
#print((f(y)/(M*g(y))))
#print("----")
if ( u <= (f(y)/(M*g(y))) ){
break
}
else{
#print("nope")
}
}
samples[i,] <- y
}
return (samples)
}
#samples <- rejection_sampling(p_scenario1, envelope_multi_norm, envelope_multi_norm_sampler, 5, 10000)
n <- 2
m <- 1000
## Uniform envelope
M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)
## Bivariate normal envelope
#M <- calculate_M(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, n)
#samples <- rejection_sampling(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, M, m, n)
#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')
acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error
# traceplot
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) +
geom_line()+
ggtitle("Trace plot of both dimensions") +
facet_wrap(~dim)
plot(g1)
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
M
