---
title: "M2 MCMC homework"
author: "Marko Ivanovski"
date: "5/8/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
#install.packages("MASS")  
#install.packages("plotly")
library("MASS")
library("plotly")
library("mvtnorm")
library(ggplot2)
library(numDeriv)
library(coda)
library(grid)
library(mcmcse)
library(stats)

#source("HMC.r")
source("Multiplot.r")

set.seed(123) 
setwd("C:/Users/marko/OneDrive/Desktop/M2/Homework5")
```
##Helping functions
```{r}
lr_likelihood <- function(y, p) {
  return ( (p**y) * ((1-p)**(1 - y)) )
}
```



## The scenarios
```{r}
p_scenario1 <- function(x){
  mu <- c(0,0)
  sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
  return (dmvnorm(x, mu, sigma))
}

p_scenario2 <- function(x) {
  B <- 0.05
  exp( (-(x[1]^2)/200- 0.5 * (x[2]+ B * x[1]^2 - 100*B)^2 ) )
}

df <- read.csv("datset.csv", header = TRUE, sep = ",")
#x are just the fit coefficients
p_scenario3 <- function(x){ 
  #df <- read.csv("datset.csv", header = TRUE, sep = ",")
  #model  <- glm(y ~ X2, family="binomial", data=df)
  #y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
  y_pred <- as.matrix(df[,1:2]) %*% x
  y_pred <- 1/(1+exp(-y_pred))
  return (prod(lr_likelihood(df$y, y_pred)))
}

p_scenario4 <- function(x){
  #model  <- glm(y ~ X2+X3+X4+X5+X6+X7+X8+X9+X10+X11, family="binomial", data=df)
  #y_pred <- predict(model, df[,2:11], type="response")
  #print(x)
  #print("---")
  y_pred <- as.matrix(df[,1:11]) %*% x
  y_pred <- 1/(1+exp(-y_pred))
  return (prod(lr_likelihood(df$y, y_pred)))
}

p_scenario3_mean <- function(){
  #df <- read.csv("datset.csv", header = TRUE, sep = ",")
  model  <- glm(y ~ X2, family="binomial", data=df)
  #print(summary(model))
  #y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
  return (model$coefficients)
}

p_scenario4_mean <- function(){
  #df <- read.csv("datset.csv", header = TRUE, sep = ",")
  model  <- glm(y ~ X2+X3+X4+X5+X6+X7+X8+X9+X10+X11, family="binomial", data=df)
  #print(summary(model))
  #y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
  return (model$coefficients)
}
```

## Rejection sampling

```{r}
envelope_multi_norm <- function(x){
  mu <- rep(0, length(x))
  sigma <- diag(nrow=length(x))
  return (dmvnorm(x, mu, sigma))
}

envelope_multi_norm_sampler <- function(n){
  g_mu <- rep(0, n)
  g_sigma <- diag(nrow=n)
  return (c(rmvnorm(1, g_mu, g_sigma)))
}

envelope_uniform <- function(x){
  return (1)
}

envelope_uniform_sampler <- function(n){
  return (c(runif(n, -5, 5)))
}

calculate_M <- function(f, g, g_sampler, n){
  samples <- matrix(vector('numeric', length=10000*n), ncol=n)
  for (i in 1:10000){
    samples[i,]<-g_sampler(n)
  }
  g_values <- apply(samples, 1, g) 
  f_values <- apply(samples, 1, f)
  
  
  M <- max(f_values/g_values, na.rm = T)
  return (M)
}

rejection_sampling <- function (f, g, g_sampler, M, m, n){
  
  samples <- matrix(vector('integer', 2*m), ncol = 2)
  for(i in 1:m){
    print(paste("On sample", i))
    repeat{
      y <- g_sampler(n) #2 is the dimension of bivariate
      u <- runif(1)
      
      #print(u)
      #print((f(y)/(M*g(y))))
      #print("----")
      if ( u <= (f(y)/(M*g(y))) ){
        break
      }
      else{
        #print("nope")
      }
    }
    samples[i,] <- y
  }
  
  return (samples)
}


#samples <- rejection_sampling(p_scenario1, envelope_multi_norm, envelope_multi_norm_sampler, 5, 10000)

```

## Sampleing from all the scenarios using REJECTION SAMPLING

### Scenario 1: sample from bivariate standard normal distirbution

```{r}
n <- 2
## Uniform envelope
set.seed(123)
#M <- calculate_M(p_scenario1, envelope_uniform, envelope_uniform_sampler, n) + 0.5 #0.5 added just to be on the safe side
#samples <- rejection_sampling(p_scenario1, envelope_uniform, envelope_uniform_sampler, M, 1000, n)

## Bivariate normal envelope
M <- calculate_M(p_scenario1, envelope_multi_norm, envelope_multi_norm_sampler, n) + 0.5
samples <- rejection_sampling(p_scenario1, envelope_multi_norm, envelope_multi_norm_sampler, M, 1000, n)


#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')


acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)

# traceplot
g1 <- ggplot( data.frame(ID = c(1:1000,1:1000), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',1000), rep('dim_2',1000)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

paste0("True mean is (0,0)")
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```
### Scenario 2: the shape of the banana function

```{r}
n <- 2
m <- 100
## Uniform envelope
#M <- calculate_M(p_scenario2, envelope_uniform, envelope_uniform_sampler, n)
#samples <- rejection_sampling(p_scenario2, envelope_uniform, envelope_uniform_sampler, M, m, n)

## Bivariate normal envelope
M <- calculate_M(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, n)
samples <- rejection_sampling(p_scenario2, envelope_multi_norm, envelope_multi_norm_sampler, M, m, n)

#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')

acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error


# traceplot
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```



### Scenario 3: the shape of the logistic regression likelihood using first two columns

```{r}
n <- 2
## Uniform envelope
#M <- calculate_M(p_scenario3, envelope_uniform, envelope_uniform_sampler, n)
#M <- 5.232946e-214
#samples <- rejection_sampling(p_scenario3, envelope_uniform, envelope_uniform_sampler, M, 10, n) #superrrrrrrr slow

## Bivariate normal envelope
#M <- calculate_M(p_scenario3, envelope_multi_norm, envelope_multi_norm_sampler, n)
M <- 1.147597e-212
m <- 100
samples <- rejection_sampling(p_scenario3, envelope_multi_norm, envelope_multi_norm_sampler, M, m, n)

#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')


acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

# traceplot
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

paste0("True mean is ", p_scenario3_mean())
paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```
## Metropolis Hasting

```{r}
metropolis_hastings <- function(f, m, x0, cov, dimensions){ 
  #New code
  samples <- matrix(vector('integer', dimensions*m), ncol = dimensions)
  samples[1,] <- x0
  for(i in 2:m){
    prev_x <-  samples[i-1,]
    new_x <- rmvnorm(1, prev_x, cov)
    alpha <- min(1, (f(c(new_x))*dmvnorm(new_x, prev_x, cov))/(f(c(prev_x))*dmvnorm(prev_x, new_x, cov)) )
    u <- runif(1)
    if (is.nan(alpha)){
      samples[i,]<-prev_x
    }
    else if (u<=alpha){
      samples[i,]<-new_x
    }
    else{
      samples[i,]<-prev_x
    }
  }
  return (samples)
}

```



## Sampleing from all the scenarios using MH SAMPLING

```{r}
n <- 2
m <- 1000
set.seed(134)
samples <- metropolis_hastings(p_scenario1, m, c(1, 1), 2*diag(2), 2)

#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')

acf(samples, lag.max = 100)

print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

# traceplot
g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

paste0("Mean of samples", mean(samples[,1])," ", mean(samples[,2]) )
```





```{r}
n <- 2
m<-1000
samples <- metropolis_hastings(p_scenario2, m, c(5.5, 4), matrix(c(2,0,0,1),nrow=2), 2)

#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface() %>% layout(title = 'Samples distribution')

acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```

```{r}
n <- 2
m<-1000
samples <- metropolis_hastings(p_scenario3, m, c(1.4, -0.76), 0.01*diag(2), 2)

#plot(samples)
#persp(den3d, box=FALSE)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()

acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```


```{r}
n <- 11
m<-1000
samples <- metropolis_hastings(p_scenario4, m, c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47), 0.005*diag(11), 11)


acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```


## HMC

```{r}
HMC = function (U, grad_U, epsilon, L, current_q)
{
  q = current_q
  p = rnorm(length(q),0,1)  # independent standard normal variates
  current_p = p

  traj <- NULL
  traj <- rbind(traj, data.frame(t(p),t(q), H = U(q)+sum(p^2) / 2))

  
  # Make a half step for momentum at the beginning
  p=p-epsilon * grad_U(q) / 2
  # Alternate full steps for position and momentum
  for (i in 1:L)
  {
    # Make a full step for the position
    q=q+epsilon * p

    # Make a full step for the momentum, except at end of trajectory
    if (i!=L) p=p-epsilon * grad_U(q)
    traj <- rbind(traj, data.frame(t(p),t(q), H = U(q)+sum(p^2) / 2))
    
  }
  # Make a half step for momentum at the end.
  p=p-epsilon * grad_U(q) / 2
  # Negate momentum at end of trajectory to make the proposal symmetric
  p=-p
  # Evaluate potential and kinetic energies at start and end of trajectory
  current_U = U(current_q)
  current_K = sum(current_p^2) / 2
  proposed_U = U(q)
  proposed_K = sum(p^2) / 2
  
  # Accept or reject the state at end of trajectory, returning either
  # the position at the end of the trajectory or the initial position

  if (runif(1) < exp(current_U-proposed_U+current_K-proposed_K))
  {
    return (list(next_q=q, traj = traj))  # accept
  }
  else
  {
    return (list(next_q=current_q, traj = traj))  # reject
  }
}
```


## Sampleing from all the scenarios using HMC

HMC (step-size and number of steps are parameters; you can use unit mass matrix, but if you want to impress, you can tune the diagonal elements as well)
```{r}
hamiltonian_sampling <- function (L, epsilon, current_q, m, minus_logf, minus_logf_grad, dim=2, visualize=TRUE){
  samples <- NULL
  for (i in 1:m) {
    #print(i)
    res = HMC(minus_logf, minus_logf_grad, epsilon, L, current_q)
    if(dim==2){
      samples = rbind(samples, data.frame(Q1 = res$next_q[1], Q2 = res$next_q[2]))
    }
    else{
      samples = rbind(samples, data.frame(
        Q1 = res$next_q[1],
        Q2 = res$next_q[2],
        Q3 = res$next_q[3],
        Q4 = res$next_q[4],
        Q5 = res$next_q[5],
        Q6 = res$next_q[6],
        Q7 = res$next_q[7],
        Q8 = res$next_q[8],
        Q9 = res$next_q[9],
        Q10 = res$next_q[10],
        Q11 = res$next_q[11]
        ))
    }
    
    current_q = res$next_q
    #if (i > 10) print(m*effectiveSize(samples[,1:2])/i) # monitor effective size of first 3 components
    
    # plot trajectory
    if (i %% 20 == 1 & visualize) {
      g1 = ggplot(res$traj,aes(x=X1,y=X2))  + coord_cartesian(ylim=c(-2, 2), xlim=c(-2,2))+ geom_point() + 
        geom_path() + theme_bw() + xlab("p1") + ylab("p2") +
        geom_point(data=res$traj[1,], colour = "red", aes(x=X1,y=X2))
      
      x <- seq(-25,25,0.2)
      x0 <- expand.grid(x,x)
      #print(x0)
      y <- apply(x0,1,minus_logf)
      df <- data.frame(x0,y = exp(-y))
      
  
  
      g2 = ggplot(res$traj,aes(x=X1.1,y=X2.1)) + geom_point() + 
        geom_path() + theme_bw() + xlab("q1")  + coord_cartesian(xlim=c(-5, 5), ylim=c(-5,5)) + ylab("q2") +
        geom_point(data=res$traj[1,], colour = "red", aes(x=X1.1,y=X2.1)) +
        geom_contour(data = df, mapping =  aes(Var1, Var2, z = y), alpha = 0.2, colour="black")  
      
      g3 = ggplot(res$traj,aes(x=1:nrow(res$traj),y=H)) + geom_point() + 
        geom_path() + theme_bw() + ylab("H") + xlab("step") 
      multiplot(g1,g2,g3,cols=3)
      
    }
  }
  return (samples)
}

```



### Scenario 1
Bivariate standard normal distribution
```{r}
minus_logf <- function(x) {
  return (-log(p_scenario1(c(x))))
}

minus_logf_grad <- function(x) {
  mu <- c(0,0)
  sigma <- matrix(c(1, 0, 0, 1), ncol = 2)
  return ((sigma%*%(x-mu)))
}
L = 10
epsilon = 0.5
#current_q = c(0,0)
current_q = c(0.5,1)
m = 1000

samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
## HMC

den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()

acf(samples, lag.max = 100)
print("MCMC standard error:")
#mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)


paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```

### Scenario 2
```{r}
B <- 0.05

minus_logf <- function(x) {
  -(-(x[1]^2)/200- 0.5 * (x[2]+ B * x[1]^2 - 100*B)^2 )
}

minus_logf_grad <- function(x) {
  g1 <- -(x[1])/100- 1.0 * (2* B * x[1]) * (x[2]+ B * x[1]^2 - 100*B)
  g2 <- - 1.0 * (x[2]+ B * x[1]^2 - 100*B)
  -c(g1,g2)
}

## HMC
L = 27
epsilon = 0.6
current_q = c(0,0) #1.4, -0.76)
m = 1000

samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)

den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()

acf(samples, lag.max = 100)
print("MCMC standard error:")
#mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)



paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```
## Scenario 3

```{r}
B <- 0.05

minus_logf <- function(x) {
  -log(p_scenario3(c(x)))
}

minus_logf_grad <- function(x) {
  df <- read.csv("datset.csv", header = TRUE, sep = ",")
  likelihood <- p_scenario3(x)
  sum_beta1 <- 0
  sum_beta2 <- 0
  for (i in 1:nrow(df)){
    exponent <- exp(-as.matrix(df[i,1:2]) %*% x)
    if (df[i,]$y==1){
      same_part <- (1+exponent)**(-1)*(-1)*exponent*(-1)
      sum_beta1 <- sum_beta1 + same_part*df[i,1]
      sum_beta2 <- sum_beta2 + same_part*df[i,2]
    }
    else{
      same_part <- (1/(1-1/(1+exponent)))*(1+exponent)**(-2)*exponent*(-1)
      sum_beta1 <- sum_beta1 + same_part*df[i,1]
      sum_beta2 <- sum_beta2 + same_part*df[i,2]
    }
  }
  #model  <- glm(y ~ X2, family="binomial", data=df)
  #y_pred <- predict(model, data.frame(X2=df[,2]), type="response")
  return (c(sum_beta1, sum_beta2))
}


minus_log_log_reg2_der <- function(x) {
  a <- unname(as.matrix(df[, names(df) == "X1" | names(df) == "X2"]))
  y <- df$y
  reg <- t(x %*% t(a))
  sigm <- 1/(1+exp(-reg))
  return (c((t((y - sigm)) %*% a)))
}

## HMC
L = 70
epsilon = 0.001
current_q = c(1.4,-0.58) #1.4 -0.6
m = 1000

samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_log_log_reg2_der)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()

acf(samples, lag.max = 100)
print("MCMC standard error:")
mcse(samples)$se #returns estimate of the mean and 
print("MCMC EES by components:")
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

g1 <- ggplot( data.frame(ID = c(1:m,1:m), x = c(samples[,1], samples[,2]), dim=c(rep('dim_1',m), rep('dim_2',m)) ), aes(x = ID, y = x)) + 
  geom_line()+
  ggtitle("Trace plot of both dimensions") +
  facet_wrap(~dim)
plot(g1)

paste0("Mean of samples (",mean(samples[,1]),",",mean(samples[,2]),")")
```



```{r}
B <- 0.05

minus_logf <- function(x) {
  -log(p_scenario4(c(x)))
}

minus_logf_grad <- function(x) {
  a <- unname(as.matrix(df[, names(data_log_reg) != "y"]))
  y <- df$y
  paste0(length(x), nrow(t(a)))
  reg <- t(x %*% t(a))
  sigm <- 1/(1+exp(-reg))
  return (c((t((y - sigm)) %*% a)))
}

## HMC
L = 50
epsilon = 0.0009
current_q = c(2.05, -0.87, -0.5,  0.72, -0.09, -1.02, -0.82,  0.24, -0.61, -0.42,  0.47) #1.4 -0.6
m = 1000

#samples <- metropolis_hastings(p_scenario4, m, , 0.000005*diag(11), 11)


samples <- hamiltonian_sampling(L, epsilon, current_q, m, minus_logf, minus_logf_grad, 11, FALSE)
ess(samples)
samples <- matrix(c(samples$Q1, samples$Q2), nrow=m)
den3d <- kde2d(samples[,1], samples[,2])
plot_ly(x=den3d$x, y=den3d$y, z=den3d$z) %>% add_surface()

acf(samples, lag.max = 100)
ess(samples)
#m * (var(samples) / m) / (mcse(samples)$se)^2 # diy ESS: sample size times ratio of MC and MCMC error

paste0(colMeans(samples))
```


