{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(gradient_function, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[np.array(x1)]\n",
    "    for i in range(1,steps+1):\n",
    "        x=all_points[i-1]-gamma*gradient_function(all_points[i-1])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def PGD(gradient_function, gamma, mu, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1, x1]\n",
    "    x=x1\n",
    "    for i in range(2,steps+2):\n",
    "        x = all_points[i-1]-gamma*gradient_function(all_points[i-1])+mu*(all_points[i-1]-all_points[i-2])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def NGD(gradient_function, gamma, mu, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1, x1]\n",
    "    x=x1\n",
    "    for i in range(2,steps+2):\n",
    "        x = all_points[i-1] - \\\n",
    "            gamma*gradient_function(all_points[i-1] + mu*(all_points[i-1]-all_points[i-2])) + \\\n",
    "            mu*(all_points[i-1]-all_points[i-2])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "        \n",
    "def AGD(gradient_function, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1]\n",
    "    all_gradients=np.ones(len(x1))\n",
    "    \n",
    "    for i in range(1,steps+1):\n",
    "        gradient_step=gradient_function(all_points[i-1])\n",
    "        D=np.diag(1/np.sqrt(all_gradients))\n",
    "        x = all_points[i-1] - gamma*D*gradient_step\n",
    "            \n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_gradients+=gradient_step**2\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewtonMethod(hessian_matrix, gradient_function, x1, steps, timelimit=None):\n",
    "    #all_points=np.zeros((steps+1, len(x1)))\n",
    "    t = time.time()\n",
    "    \n",
    "    all_points=[x1]\n",
    "    for i in range(1,steps+1):\n",
    "        x=all_points[i-1]-np.dot(np.linalg.inv(hessian_matrix(all_points[i-1])),gradient_function(all_points[i-1]))\n",
    "        #print(x)\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        #all_points[i,:]=x\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points\n",
    "\n",
    "def BFGS(gradient_function, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    \n",
    "    B=np.eye(len(x1))\n",
    "    all_points=[x1, x1-np.dot(B,gradient_function(x1))]\n",
    "    \n",
    "    for i in range(steps):\n",
    "        xk, xk_1 = all_points[-1], all_points[-2]\n",
    "        grad_xk, grad_xk_1 = gradient_function(xk), gradient_function(xk_1)\n",
    "        \n",
    "        gamma = grad_xk-grad_xk_1\n",
    "        delta = xk-xk_1\n",
    "        \n",
    "        #print(all_points)\n",
    "        #print(grad_xk)\n",
    "        #print(grad_xk_1)\n",
    "        gamma=gamma[..., None]\n",
    "        delta=delta[..., None]\n",
    "        \n",
    "        if delta.T.dot(gamma) == 0:\n",
    "            print('    BFGS OVERFLOW!!!')\n",
    "            return all_points\n",
    "        \n",
    "        B_new = B - (delta.dot(gamma.T.dot(B)) + B.dot(gamma).dot(delta.T)) / (delta.T.dot(gamma)) + \\\n",
    "             (1 + (gamma.T.dot(B).dot(gamma)) / (delta.T.dot(gamma))) * (delta * delta.T) / (delta.T.dot(gamma))      \n",
    "        \n",
    "        all_points.append(xk-B_new.dot(gradient_function(xk)))\n",
    "        B=B_new\n",
    "        \n",
    "        #print(time.time() - t)\n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 solution checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(-1), 1, -1, 1, -1, 1, -1, 1, -1, 1, -1]\n",
      "[0, 0, -1.0, 0.0, 0.0, -1.0, 0.0, 0.0, -1.0, 0.0, 0.0, -1.0]\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_gd(x):\n",
    "    return 2*x\n",
    "\n",
    "print(GD(gradient_function_gd, 1, -1, 10))\n",
    "\n",
    "def gradient_function_pgd(x):\n",
    "    return 6*x+2\n",
    "\n",
    "print(PGD(gradient_function_pgd, 0.5, 1, 0, 10))\n",
    "\n",
    "def gradient_function_ngd(x):\n",
    "    return 2*x-1\n",
    "\n",
    "##print(NGD(gradient_function_ngd, 2/3, 1, 0, 1 , 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def commpare_all(hessian_function, gradient_function, x1, gamma, mu, actual_min):\n",
    "    \n",
    "    print(f\"Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 2)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 2)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 2)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 2)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    \n",
    "    \n",
    "    print(f\"Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 5)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 5)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 5)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 5)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 5)[-1]- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 10)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 10)[-1]- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 100)[-1]- actual_min)):.5f}\")\n",
    "    \n",
    "    print(\"Testing on time now...\")\n",
    "    print(f\"Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 1000000000000000000, 0.1)[-1]- actual_min))}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 1000000000000000000, 0.1)[-1]- actual_min))}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 1000000000000000000, 0.1)[-1]- actual_min))}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 1000000000000000000, 0.1)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 1000000000000000000, 0.1)[-1]- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 10000000000000000000000000, 1)[-1]- actual_min))}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 10000000000000000000000000, 1)[-1]- actual_min))}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 10000000000000000000000000, 1)[-1]- actual_min))}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 10000000000000000000000000, 1)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 10000000000000000000000000, 1)[-1]- actual_min))}\")\n",
    "    \n",
    "    print(f\"Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\")\n",
    "    print(f\"    Gradient descend: {np.sum(np.square(GD(gradient_function, gamma, x1, 10000000000000000000000000, 2)[-1]- actual_min))}\")\n",
    "    print(f\"    Polyak gradient: {np.sum(np.square(PGD(gradient_function, gamma, mu, x1, 10000000000000000000000000, 2)[-1]- actual_min))}\")\n",
    "    print(f\"    Nestorov gradient descend: {np.sum(np.square(NGD(gradient_function, gamma, mu, x1, 10000000000000000000000000, 2)[-1]- actual_min))}\")\n",
    "    print(f\"    Newton method: {np.sum(np.square(NewtonMethod(hessian_function, gradient_function, x1, 10000000000000000000000000, 2)[-1]- actual_min))}\")\n",
    "    print(f\"    BFGS method: {np.sum(np.square(BFGS(gradient_function, x1, 10000000000000000000000000, 2)[-1]- actual_min))}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function on which we are going to compare all approaches is:\n",
    "$$f(x,y,z)=(x-z)^{2}+(2y+z)^{2}+(4x-2y+z)^{2}+x+y$$\n",
    "First we will start with \n",
    "$$x_{1}=(0,0,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.09337\n",
      "    Polyak gradient: 0.09301\n",
      "    Nestorov gradient descend: 0.09303\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS method: 8.520524277920641e-16\n",
      "Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.07586\n",
      "    Polyak gradient: 0.07473\n",
      "    Nestorov gradient descend: 0.07478\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS method: 0.046619878826407814\n",
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.05475\n",
      "    Polyak gradient: 0.05305\n",
      "    Nestorov gradient descend: 0.05311\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS method: 8.520524277920641e-16\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00027\n",
      "    Polyak gradient: 0.00019\n",
      "    Nestorov gradient descend: 0.00020\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "Testing on time now...\n",
      "Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Newton method: 1.5407439555097887e-33\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 2.311115933264683e-33\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_a_part(X):\n",
    "    x,y,z = X[0], X[1], X[2]\n",
    "    return np.array([34*x-16*y+6*z+1, -16*x+16*y+1, 6*x+6*z])\n",
    "\n",
    "def hessian_function_a_part(_):\n",
    "    return np.array([\n",
    "        np.array([34, -16, 6]),\n",
    "        np.array([-16, 16, 0]),\n",
    "        np.array([6, 0, 6]),\n",
    "    ])\n",
    "\n",
    "gamma = 0.01\n",
    "mu = 0.05\n",
    "x1=np.array([0,0,0])\n",
    "actual_min = np.array([-1/6, -11/48, 1/6])\n",
    "\n",
    "commpare_all(hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then \n",
    "$$x_{1}=(1,1,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 2.19513\n",
      "    Polyak gradient: 2.17947\n",
      "    Nestorov gradient descend: 2.18129\n",
      "    Newton method: 0.0\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 1.58360\n",
      "    Polyak gradient: 1.54683\n",
      "    Nestorov gradient descend: 1.54945\n",
      "    Newton method: 2.311115933264683e-33\n",
      "    BFGS method: 1.4411677504448062e-09\n",
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.99750\n",
      "    Polyak gradient: 0.95503\n",
      "    Nestorov gradient descend: 0.95750\n",
      "    Newton method: 2.311115933264683e-33\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.00407\n",
      "    Polyak gradient: 0.00295\n",
      "    Nestorov gradient descend: 0.00298\n",
      "    Newton method: 0.00000\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.00000\n",
      "Testing on time now...\n",
      "Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Newton method: 2.311115933264683e-33\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Newton method: 2.311115933264683e-33\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 2.311115933264683e-33\n",
      "Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 5.9241605089351375e-31\n",
      "    Time limit reached!\n",
      "    Newton method: 2.311115933264683e-33\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 2.311115933264683e-33\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "mu = 0.05\n",
    "x1=np.array([1,1,0])\n",
    "actual_min = np.array([-1/6, -11/48, 1/6])\n",
    "\n",
    "commpare_all(hessian_function_a_part, gradient_function_a_part, x1, gamma, mu, actual_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function on which we are going to compare all approaches is:\n",
    "$$(x-1)^{2}+(y-1)^{2}+100(y-x^{2})^{2}+100(z-y^{2})^{2}$$\n",
    "First we will start with \n",
    "$$x_{1}=(1.2, 1.2, 1.2)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.11045\n",
      "    Polyak gradient: 0.10999\n",
      "    Nestorov gradient descend: 0.11004\n",
      "    Newton method: 0.06862321660838383\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.10119\n",
      "    Polyak gradient: 0.10003\n",
      "    Nestorov gradient descend: 0.10014\n",
      "    Newton method: 0.000168833131814448\n",
      "    BFGS method: 1.5086514831559803e+50\n",
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.09315\n",
      "    Polyak gradient: 0.09202\n",
      "    Nestorov gradient descend: 0.09213\n",
      "    Newton method: 0.0\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 0.08542\n",
      "    Polyak gradient: 0.08532\n",
      "    Nestorov gradient descend: 0.08533\n",
      "    Newton method: 0.00000\n",
      "    BFGS method: nan\n",
      "Testing on time now...\n",
      "Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 0.03160469616565139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-216-9f1823781d78>:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.array([2*(x-1)-400*x*(y-x**2), 2*(y-1)+200*(y-x**2)-400*y*(z-y**2), 200*(z-y**2)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Time limit reached!\n",
      "    Polyak gradient: 0.02925397863682173\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.038286533427595804\n",
      "    Time limit reached!\n",
      "    Newton method: 0.0\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 6.469020925437517e-07\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 2.692928584785769e-06\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 1.2434498464866268e-05\n",
      "    Time limit reached!\n",
      "    Newton method: 0.0\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 1.0122531610683364e-12\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 1.0688468814207739e-11\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 3.4587406090219427e-10\n",
      "    Time limit reached!\n",
      "    Newton method: 0.0\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n"
     ]
    }
   ],
   "source": [
    "#Second function\n",
    "def gradient_function_b_part(X):\n",
    "    x, y, z = X[0], X[1], X[2]\n",
    "    return np.array([2*(x-1)-400*x*(y-x**2), 2*(y-1)+200*(y-x**2)-400*y*(z-y**2), 200*(z-y**2)])\n",
    "\n",
    "def hessian_function_b_part(X):\n",
    "    x, y, z = X[0], X[1], X[2]\n",
    "    return np.array([\n",
    "        [-400*(y-x**2) + 800*x**2 + 2, -400*x, 0],\n",
    "        [-400*x, -400*(z-y**2) + 800*y**2 + 202, -400*y],\n",
    "        [0, -400*y, 200]\n",
    "    ])\n",
    "\n",
    "\n",
    "gamma = 0.0001\n",
    "mu = 0.1\n",
    "x1=np.array([1.2, 1.2, 1.2])\n",
    "actual_min = np.array([1, 1, 1])\n",
    "\n",
    "commpare_all(hessian_function_b_part, gradient_function_b_part, x1, gamma, mu, actual_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then,\n",
    "$$x_{1}=(-1, 1.2, 1.2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.12931\n",
      "    Polyak gradient: 4.13204\n",
      "    Nestorov gradient descend: 4.13161\n",
      "    Newton method: 4.09248\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.18023\n",
      "    Polyak gradient: 4.18757\n",
      "    Nestorov gradient descend: 4.18636\n",
      "    Newton method: 3.57209\n",
      "    BFGS method: 2.4892343743311203e+79\n",
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.22509\n",
      "    Polyak gradient: 4.23184\n",
      "    Nestorov gradient descend: 4.23078\n",
      "    Newton method: 877.49327\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.24667\n",
      "    Polyak gradient: 4.24539\n",
      "    Nestorov gradient descend: 4.24542\n",
      "    Newton method: 0.00000\n",
      "    BFGS method: nan\n",
      "Testing on time now...\n",
      "Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 3.6940286868507286\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 3.6925588314917706\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 3.702598699300195\n",
      "    Time limit reached!\n",
      "    Newton method: 0.0\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 3.3898987471825195e-05\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 0.00011944492079249986\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.0005082420541028935\n",
      "    Time limit reached!\n",
      "    Newton method: 0.0\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 3.510846560542728e-11\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 4.391484727976158e-10\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 3.481750028988994e-08\n",
      "    Time limit reached!\n",
      "    Newton method: 0.0\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.0001\n",
    "mu = 0.1\n",
    "x1=np.array([-1, 1.2, 1.2])\n",
    "actual_min = np.array([1, 1, 1])\n",
    "\n",
    "commpare_all(hessian_function_b_part, gradient_function_b_part, x1, gamma, mu, actual_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first function on which we are going to compare all approaches is:\n",
    "$$(1.5-x+xy)^{2}+(2.25-x+xy^{2})^{2}+(2.625-x+xy^{3})^{2}$$\n",
    "First we will start with \n",
    "$$x_{1}=(1, 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.24447\n",
      "    Polyak gradient: 4.24419\n",
      "    Nestorov gradient descend: 4.24419\n",
      "    Newton method: 9.25\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.23620\n",
      "    Polyak gradient: 4.23502\n",
      "    Nestorov gradient descend: 4.23501\n",
      "    Newton method: 9.25\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 4.22247\n",
      "    Polyak gradient: 4.21979\n",
      "    Nestorov gradient descend: 4.21978\n",
      "    Newton method: 9.25\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 3.98408\n",
      "    Polyak gradient: 3.95600\n",
      "    Nestorov gradient descend: 3.95591\n",
      "    Newton method: 9.25000\n",
      "    BFGS method: nan\n",
      "Testing on time now...\n",
      "Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 0.33608691345222635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-217-2b80ffedcb74>:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return np.array([2*(1.5-x+x*y)*(y-1)+2*(2.25-x+x*y**2)*(y**2-1)+2*(2.625-x+x*y**3)*(y**3-1), \\\n",
      "<ipython-input-217-2b80ffedcb74>:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*(x*y)+6*(2.625-x+x*y**3)*(x*y**2)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Time limit reached!\n",
      "    Polyak gradient: 0.3072808506784999\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.3890290195882501\n",
      "    Time limit reached!\n",
      "    Newton method: 9.25\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 0.0007257447947076049\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 0.0005181942929336989\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.0011661545534209724\n",
      "    Time limit reached!\n",
      "    Newton method: 9.25\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 2.070774620628967e-06\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 1.9138550265357493e-06\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 7.310316779653059e-06\n",
      "    Time limit reached!\n",
      "    Newton method: 9.25\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n"
     ]
    }
   ],
   "source": [
    "def gradient_function_c_part(X):\n",
    "    x, y = X[0], X[1]\n",
    "    return np.array([2*(1.5-x+x*y)*(y-1)+2*(2.25-x+x*y**2)*(y**2-1)+2*(2.625-x+x*y**3)*(y**3-1), \\\n",
    "                    2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*(x*y)+6*(2.625-x+x*y**3)*(x*y**2)])\n",
    "\n",
    "def hessian_matrix_c_part(X):\n",
    "    x, y = X[0], X[1]\n",
    "    return np.array([[2*(y**6+y**4-2*y**3-y**2-2*y+3), 2*x*(6*y**5+4*y**3-6*y**2-2*y-2)+15.75*y**2+9*y+3], \\\n",
    "                    [2*x*(6*y**5+4*y**3-6*y**2-2*y-2)+15.75*y**2+9*y+3, 2*x*(x*(15*y**4+6*y**2-6*y-1)+6*2.625*y+4.5*y)]])\n",
    "\n",
    "#x1=(1,1,0)\n",
    "\n",
    "\n",
    "gamma = 0.0001\n",
    "mu = 0.1\n",
    "x1=np.array([1,1])\n",
    "actual_min = np.array([3, 0.5])\n",
    "\n",
    "commpare_all(hessian_matrix_c_part, gradient_function_c_part, x1, gamma, mu, actual_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then\n",
    "$$x_{1}=(4.5, 4.5)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing all the methods on 2 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 14.81387\n",
      "    Polyak gradient: 13.89379\n",
      "    Nestorov gradient descend: 14.08709\n",
      "    Newton method: 7.91576\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 5 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 12.15934\n",
      "    Polyak gradient: 9.39337\n",
      "    Nestorov gradient descend: 9.96291\n",
      "    Newton method: 3.52364\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 10 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 9.82881\n",
      "    Polyak gradient: 6.85520\n",
      "    Nestorov gradient descend: 7.32214\n",
      "    Newton method: 8.58271\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 100 steps, shown bellow is the distance from the actual minimum x*\n",
      "    Gradient descend: 3.58792\n",
      "    Polyak gradient: 2.41752\n",
      "    Nestorov gradient descend: 2.46649\n",
      "    Newton method: 9.25000\n",
      "    BFGS method: nan\n",
      "Testing on time now...\n",
      "Testing all the methods on 0.1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 0.6523582728439231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-2b80ffedcb74>:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return np.array([2*(1.5-x+x*y)*(y-1)+2*(2.25-x+x*y**2)*(y**2-1)+2*(2.625-x+x*y**3)*(y**3-1), \\\n",
      "<ipython-input-28-2b80ffedcb74>:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  2*(1.5-x+x*y)*x+4*(2.25-x+x*y**2)*(x*y)+6*(2.625-x+x*y**3)*(x*y**2)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Time limit reached!\n",
      "    Polyak gradient: 0.591760579479422\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.6130932251854818\n",
      "    Time limit reached!\n",
      "    Newton method: 9.25\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 1 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 0.6210439526028628\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 0.5685557506095547\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.5886862503580663\n",
      "    Time limit reached!\n",
      "    Newton method: 9.25\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n",
      "Testing all the methods on 2 seconds, shown bellow is the distance from the actual minimum x*\n",
      "    Time limit reached!\n",
      "    Gradient descend: 0.6064765047218923\n",
      "    Time limit reached!\n",
      "    Polyak gradient: 0.5493178759305221\n",
      "    Time limit reached!\n",
      "    Nestorov gradient descend: 0.5701152965926732\n",
      "    Time limit reached!\n",
      "    Newton method: 9.25\n",
      "    Time limit reached!\n",
      "    BFGS method: nan\n"
     ]
    }
   ],
   "source": [
    "gamma = 1e-6\n",
    "mu = 0.5\n",
    "x1=np.array([4.5, 4.5])\n",
    "actual_min = np.array([3, 0.5])\n",
    "\n",
    "commpare_all(hessian_matrix_c_part, gradient_function_c_part, x1, gamma, mu, actual_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first define SGD and L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(_, gamma, x1, steps, timelimit=None):\n",
    "    t = time.time()\n",
    "    all_points=[np.array(x1)]\n",
    "    \n",
    "    intercept=x1[0]\n",
    "    beta=x1[1]\n",
    "    n=X.shape[0]\n",
    "    \n",
    "    for i in range(1,steps+1):\n",
    "        j = np.random.randint(0, X.shape[0])\n",
    "        instance=X[j,:]\n",
    "        label=y[j]\n",
    "        \n",
    "        intercept=all_points[-1][0]\n",
    "        beta=all_points[-1][1]\n",
    "        \n",
    "        x=all_points[-1]-gamma*np.array([2/n*(beta*instance[1]+intercept-label), 2/n*(beta*instance[1]+intercept-label)*instance[1]])\n",
    "        #x=pd(x) #if necessery add PD function\n",
    "        all_points.append(x)\n",
    "        \n",
    "        if timelimit != None and time.time() - t > timelimit:\n",
    "                print('    Time limit reached!')\n",
    "                break\n",
    "        \n",
    "    return all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_points(N):\n",
    "    random.seed(0)\n",
    "    x=np.array([i for i in range(1,N+1)])\n",
    "    y=np.array([i+random.uniform(0, 1) for i in range(1,N+1)])\n",
    "    return np.insert(x[...,None], 0, np.ones(x.shape[0]), axis=1), y\n",
    "\n",
    "\n",
    "X, y = generate_points(100)\n",
    "\n",
    "plt.plot(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00060802]\n",
      "0.5549681217666134\n"
     ]
    }
   ],
   "source": [
    "linr_model = LinearRegression().fit(X[:,1][...,None], y)\n",
    "print(linr_model.coef_)\n",
    "print(linr_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(params):\n",
    "    y_pred=np.dot(X, params[...,None])\n",
    "    n=X.shape[0]\n",
    "    return np.sum((y_pred-y[...,None])**2)/n\n",
    "\n",
    "def gradf(params):\n",
    "    y_pred=np.dot(X, params[...,None])\n",
    "    residuals=y_pred - y[...,None]\n",
    "    n=X.shape[0]\n",
    "    return np.array([2/n*np.sum(residuals), 2/n*np.dot(residuals[:,0], X[:,1])])\n",
    "\n",
    "def Hf(_):\n",
    "    n=X.shape[0]\n",
    "    return np.array([\n",
    "        [2, 2/n*np.sum(X[:,1])],\n",
    "        [2/n*np.sum(X[:,1]), 2/n*np.sum(X[:,1]**2)]\n",
    "    ])\n",
    "\n",
    "\n",
    "def f_lbfgs(params, *args):\n",
    "    X, y = args[0], args[1]\n",
    "    return f(params)\n",
    "\n",
    "def gradf_lbfgs(params, *args):\n",
    "    X, y = args[0], args[1]\n",
    "    return gradf(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def fit_model(N, hessian_function, gradient_function, x1, gamma, gamma_sgd, steps):\n",
    "    X, y = generate_points(N)\n",
    "    \n",
    "    params = fmin_l_bfgs_b(f_lbfgs, fprime=gradf_lbfgs, x0=x1, approx_grad=False, args=(X,y), maxfun=steps)\n",
    "    \n",
    "    print(f\"Fitting a linear regression model using all methods on {N} points, using {steps} steps...\")\n",
    "    print(f\"    Gradient descend: {GD(gradient_function, gamma, x1, steps)[-1]}\")\n",
    "    print(f\"    Stochastic descend: {SGD(_, gamma_sgd, x1, steps)[-1]}\")\n",
    "    print(f\"    Newton method: {NewtonMethod(hessian_function, gradient_function, x1, steps)[-1]}\")\n",
    "    print(f\"    BFGS method: {BFGS(gradient_function, x1, steps)[-1]}\")\n",
    "    print(f'    LBFGS method:[{params[0]}]')\n",
    "    \n",
    "    print(f\"And the function values...\")\n",
    "    print(f\"    Gradient descend: {f(GD(gradient_function, gamma, x1, steps)[-1])}\")\n",
    "    print(f\"    Stochastic descend: {f(SGD(_, gamma_sgd, x1, steps)[-1])}\")\n",
    "    print(f\"    Newton method: {f(NewtonMethod(hessian_function, gradient_function, x1, steps)[-1])}\")\n",
    "    print(f\"    BFGS method: {f(BFGS(gradient_function, x1, steps)[-1])}\")\n",
    "    print(f'    LBFGS method:[{f(params[0])}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting model to points generated as:\n",
    "$${(i, i+Uniform(0,1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up we start by generating 50 points and testing all methods how they perform when we do 50 or 1000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 50 points, using 50 steps...\n",
      "    Gradient descend: [0.01473602 0.97833451]\n",
      "    Stochastic descend: [0.00050451 0.03428872]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 3.255401915102025\n",
      "    Stochastic descend: 3214.696907787424\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0,0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(50, Hf, gradf, x1, gamma, gamma_sgd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 50 points, using 1000 steps...\n",
      "    Gradient descend: [0.01770798 1.00862741]\n",
      "    Stochastic descend: [0.01912556 1.00876245]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 0.1407313856281898\n",
      "    Stochastic descend: 0.14408289274225114\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.001\n",
    "fit_model(50, Hf, gradf, x1, gamma, gamma_sgd, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate 100 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 100 points, using 50 steps...\n",
      "    Gradient descend: [0.01473602 0.97833451]\n",
      "    Stochastic descend: [0.00048258 0.03371817]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 3.255401915102025\n",
      "    Stochastic descend: 3236.9948419580546\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(100, Hf, gradf, x1, gamma, gamma_sgd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 100 points, using 1000 steps...\n",
      "    Gradient descend: [0.01770798 1.00862741]\n",
      "    Stochastic descend: [0.00749578 0.50067931]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 0.1407313856281898\n",
      "    Stochastic descend: 873.9409662364535\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(100, Hf, gradf, x1, gamma, gamma_sgd, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate 1000 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 1000 points, using 50 steps...\n",
      "    Gradient descend: [0.01473602 0.97833451]\n",
      "    Stochastic descend: [0.00056128 0.03936011]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 3.255401915102025\n",
      "    Stochastic descend: 3223.475301462714\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(1000, Hf, gradf, x1, gamma, gamma_sgd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 1000 points, using 1000 steps...\n",
      "    Gradient descend: [0.01770798 1.00862741]\n",
      "    Stochastic descend: [0.00739682 0.49484662]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 0.1407313856281898\n",
      "    Stochastic descend: 945.9570181642692\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(1000, Hf, gradf, x1, gamma, gamma_sgd, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate 10 000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 10000 points, using 50 steps...\n",
      "    Gradient descend: [0.01473602 0.97833451]\n",
      "    Stochastic descend: [0.00047539 0.03015042]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 3.255401915102025\n",
      "    Stochastic descend: 3224.482637474414\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(10000, Hf, gradf, x1, gamma, gamma_sgd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 10000 points, using 1000 steps...\n",
      "    Gradient descend: [0.01770798 1.00862741]\n",
      "    Stochastic descend: [0.00742746 0.49020822]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 0.1407313856281898\n",
      "    Stochastic descend: 890.5882406923513\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(10000, Hf, gradf, x1, gamma, gamma_sgd, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we generate a million of them :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 1000000 points, using 50 steps...\n",
      "    Gradient descend: [0.01473602 0.97833451]\n",
      "    Stochastic descend: [0.00049836 0.0337737 ]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 3.255401915102025\n",
      "    Stochastic descend: 3225.8102505769343\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(1000000, Hf, gradf, x1, gamma, gamma_sgd, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a linear regression model using all methods on 1000000 points, using 1000 steps...\n",
      "    Gradient descend: [0.01770798 1.00862741]\n",
      "    Stochastic descend: [0.00748781 0.49527008]\n",
      "    Newton method: [0.55496812 1.00060802]\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: [0.55496812 1.00060802]\n",
      "    LBFGS method:[[0.55496813 1.00060802]]\n",
      "And the function values...\n",
      "    Gradient descend: 0.1407313856281898\n",
      "    Stochastic descend: 903.980950619941\n",
      "    Newton method: 0.06964631475088329\n",
      "    BFGS OVERFLOW!!!\n",
      "    BFGS method: 0.06964631475088344\n",
      "    LBFGS method:[0.0696463147508833]\n"
     ]
    }
   ],
   "source": [
    "x1=np.array([0, 0])\n",
    "gamma=0.00001\n",
    "gamma_sgd=0.00001\n",
    "fit_model(1000000, Hf, gradf, x1, gamma, gamma_sgd, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "For the first function:\n",
    "$$f(x,y,z)=(x-z)^{2}+(2y+z)^{2}+(4x-2y+z)^{2}+x+y x_{1}=(0,0,0)$$\n",
    "One can definetly say that Newton and BFGS performed the best when tested on steps, although granted, we started close to the minimum. Then comes Polyak which was a bit better than Nestorov and finaly the normal Gradient Descend method. Then I tested all methods on different time intervals. When the time was very little, Newton was the best although BFGS was not far behind. Finally, on 2 seconds, Newton came the closes while GD, PGD and NGD all came to the same x. Similar results can be concluded when we started at point:\n",
    "$$x_{1}=(1,1,0)$$\n",
    "\n",
    "Then comes the second function:\n",
    "$$(x-1)^{2}+(y-1)^{2}+100(y-x^{2})^{2}+100(z-y^{2})^{2} x_{1}=(1.2, 1.2, 1.2)$$\n",
    "Once again, the comparison is quite similar. Newton was the closest and when it performed 5 steps it came exactly at the global minimum. BFGS had a lot of issues with overflowing and limited space hence L-BFGS would be a potential alternative. Interestingly when given more time, Nestorov performed a little better than Polyak. \n",
    "\n",
    "Finally the third function:\n",
    "$$(1.5-x+xy)^{2}+(2.25-x+xy^{2})^{2}+(2.625-x+xy^{3})^{2} x_{1}=(1, 1)$$ \n",
    "BFGS had a overflow issues. Interestingly Newton was not the best it got stuck in a point. All other methods were very similar with Polyak having a slight advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "When fitting parameters to N points using 50 steps SGD did a poor job, GD had a big loss as well and BFGS, LBFGS did a great job. Newton was in the middle. When I increased the number of steps to 1000 GD and SGD did a better job. All three Newton, BFGS and LBFGS returned almost the same solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ids-clone] *",
   "language": "python",
   "name": "conda-env-ids-clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
